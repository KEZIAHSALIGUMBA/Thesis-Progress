---
title: "Application of GMM"
author: "Keziah M. Saligumba"
date: "2024-09-20"
categories: [concepts, application]
image: "thumbnail.png"
bibliography: reference.bib
nocite: |
  @gradient, @textidentification
---

*Previously,* it was tackled that Gaussian Mixture Model are powerful probabilistic models for density estimation and is known for it's flexibility in understanding data distributions. *In this blog, we'll explore how Gaussian Mixture Model work, particularly in the context of it's applications, and it's key concepts.*

## GMM & EM Algorithm

According to @bishopcprml, Gaussian mixture model are widely used in data mining, pattern recognition, machine learning, and statistical analysis.

The *Expectation-Maximization* algorithm is a method used to find the maximum likelihood estimate in models that contain latent variables. It is used because it is not possible to solve the derivative of the in close-form which is

$$
\log p(x|\pi, \mu, \Sigma) = \sum_{n=1}^N \log \left\{ \sum_{k=1}^K \pi_k N(x_k|\mu_k,\Sigma_k) \right\}
$$

The first step of this algorithm is called *Expectation Step*, which assumes that the current parameter estimates are fixed. It is used to compute the expected values of the latent variable in the model. The algorithm is then followed by a *Maximization Step* that takes the expected values of the latent variables and finds updated values for the previous parameter estimates that maximize the likelihood function [@emmix].

In the case of Gaussian mixture model, the *Expectation Step* assumes that the values of all 3 parameters $(\pi, \mu, \Sigma)$ are fixed and then computes the probability of each data point that belongs to each Gaussian component. Once the probability of each data point is calculated, it is then called "*responsibility*", which indicates how much each Gaussian contributes to the overall distribution of the data. After calculating the responsibilities, we then proceed to *Maximization* *Step* where we update the parameters based on these responsibilities. In this step, it maximizes the likelihood of the data given the current model parameters.

Through these step-by-step methods, it converges to a set of parameters that best describe the data, enable us to do effective classifications.

To have a further context related to *EM algo*rithm, you can visit [MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION](https://pearlclave2002.github.io/Thesis2k24/posts/post-with-code/)

## Application of GMMs

As stated, Gaussian Mixture Model is widely used in machine learning, and find significant applications particularly in classification tasks. One of these is the **Handwritten Digit Recognition.**

### Handwritten Digit Recognition

In recent years, machine learning techniques when applied to neural networks have played a significant role in pattern recognition such as recognizing handwritten digits. Recognizing handwritten digits is one of a big challenge for computers. The ***Modified National Institute of Standards and Technology (MNIST)*** database contains thousands of handwritten digits that commonly used for classification methods. The use of Gaussian Mixture Model provides models on how different people write the same digits by understanding patterns in pixel intensities.

Using advanced features like Histogram of Gradients can help manage to determine how gradients change across the images, and improves performance by capturing important details such as strokes, and shapes of the digits. In addition, the *Expectation-Maximization algorithm* serves a significant role also in Handwritten Digit Recognition. Consider the digits 0-9 as a model which each of these digits are Gaussian distributed.

In *EM algorithm,* this is how we can divide the recognition:

**Expectation Step:**

Let's say from MNIST database, we take an image of the digit "1". In this step, we calculate the probability of how likely this image belongs to each of the Gaussian distributions representing the digits from 0 to 9. The algorithm assesses how likely it is that the pixel intensity pattern of this image fit each digit's model. This figures out which Gaussian best describes the observed data by determining the "responsibility" of each Gaussian for the image.

**Maximization Step:**

Once the responsibility are calculated, the algorithm updates the parameters of each Gaussian distribution which was determined during E-Step. This maximizes the likelihood of the data under the current model.
