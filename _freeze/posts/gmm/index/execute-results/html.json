{
  "hash": "fb37efc367c76e04e3a599da0c49ce99",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Gaussian Mixture Model\"\nauthor: \"Keziah M. Saligumba\"\ndate: \"2024-09-15\"\ncategories: [concepts]\nimage: \"thumbnail.png\"\nbibliography: reference.bib\nnocite: |\n  @bishopcprml, @bernstein2021gmm_em\n---\n\n\n\n\n*In our previous discussion, we discussed about Mixture Models, where probabilities are used to determine what components the observations will most likely belong. In this section, we will explore a certain type of Mixture Models where it uses Normal Distributions, known as **Gaussian Mixture Model***.\n\n## Gaussian Mixture Model\n\nOne of the common type of mixture models is the **Gaussian Mixture Model (GMM)**. The term **Gaussian** comes from the German mathematician and physicist Carl Friedrich Gauss, who is particularly known for his work on the **Normal Distribution** - often referred to as the ***Gaussian distribution***.\n\n## Definition\n\nIn **Gaussian Mixture Model**, we assume that the data is generated from a mixture of $K$ Gaussian distributions. To generate a data point $X$, we first select one of these $K$ Gaussian according to Categorical distribution:\n\n$$\nZ\\sim Cat(\\pi_1, \\pi_2, \\cdots, \\pi_K)\n$$\n\nwhere $Z\\in\\left\\{ 1,2, \\cdots, K \\right\\}$ indicates which Gaussian is chosen. When a particular element is selected, this element is denoted by $Z_k$ and is equal to 1, and all other elements are equal to 0. The values of these element satisfies $Z_k\\in\\left\\{ 0,1\\right\\}$ and $\\sum_{k=1}^K Z_k =1$. Such that $p(Z_k =1)=\\pi_k$ is the marginal distribution over $Z$, where $\\pi_k$ is the **mixing coefficient** or ***mixing weight***. \n\nOnce that $Z=k$ , the data point $X$ is then sampled from the $k$th Gaussian:\n\n$$X \\sim N(\\mu_k, \\Sigma_k)$$\n\nwhere $\\mu_k$ and $\\Sigma_k$ are the mean and covariance matrix respectively of the selected Gaussian component. The latent variable $Z$, which is not directly observed, indicates which Gaussian component generated each data point $X$.\n\nFor a **univariate normal distribution**, the data is one-dimensional (e.g., height or weight), it is characterized by two key parameters:\n\n$\\bullet$ Mean ($\\mu$): which is the average value, represents the central value where data points are likely to be found\n\n$\\bullet$ Variance($\\sigma^2$): where it determines the spread of the data points around the mean\n\nThe Gaussian Distribution has a density function of:\n\n$$f(x) = f_x(x;\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-(x-\\mu)^2/2\\sigma^2}$$\n\nFrom the law of total probability,\n\n$$P(A) = \\sum P(A \\cap B_k)$$ the marginal probability of $X$ is:\n\n$$P(X = x) = \\sum_{k=1}^K P(X=x|Z=k)\\underbrace{P(Z=k)}_{\\pi_k} = \\sum_{k=1}^K P(X=x|Z=k)\\pi_k$$\n\nwhere $\\sum_{k=1}^K \\pi_k =1$ ensures that the mixing weight sum to 1 (*as stated previously*). The mixture component is represented as $P(X |Z = k)$.\n\nThe Probability Mass Function for discrete random variables of the mixture model is:\n\n$$p(x) =  \\sum_{k=1}^{K}\\pi_k p(x \\mid Z_{k})$$\n\nor could also be written as\n\n$$p(x) =  \\sum_{k=1}^{K}\\pi_k \\cdot f(x \\mid \\mu_k, \\sigma_k^2)$$\n\nFor a **multivariate normal distribution** where the data is from a multiple dimensions, the probability density function for continuous random variables of the mixture model is:\n\n$$f_{x}(x) = \\sum_{k=1}^{K}\\pi_k f_{x \\mid Z_{k}}(x \\mid Z_{k}) $$ or could also be written as\n\n$$p(x) = \\sum_{k=1}^{K} \\pi_k \\cdot f(x \\mid \\mu_k, \\Sigma_k)$$ where $f(x \\mid \\mu_k, \\Sigma_k)$ is normal distribution with mean vector $\\mu_k$ and covariance matrix $\\sum_k$.\n\n## Visualizing GMM\n\nNow assume our data are the heights of students at the University of Chicago. Assume the height of a randomly chosen male is normally distributed with a mean equal to 5′9 and a standard deviation of 2.5 inches and the height of a randomly chosen female is $N$(5′4,2.5).However, instead of 50/50 mixture proportions, assume that 75% of the population is female, and 25% is male. We simulate heights in a similar fashion as above, with the corresponding changes to the parameters:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNUM.SAMPLES <- 5000\nheights      <- numeric(NUM.SAMPLES)\nfor(i in seq_len(NUM.SAMPLES)) {\n  z.i <- rbinom(1,1,0.75)\n  if(z.i == 0) heights[i] <- rnorm(1, mean = 69, sd = 2.5)\n  else heights[i] <- rnorm(1, mean = 64, sd = 2.5)\n}\nhist(heights)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nNow we see that histogram is unimodal. Are heights normally distributed under this model? We plot the corresponding densities below:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nHere we see that the Gaussian mixture model is unimodal because there is so much overlap between the two densities. In this example, you can see that the population density is not symmetric, and therefore not normally distributed.\n\nThese two illustrative examples above give rise to the general notion of a mixture model which assumes each observation is generated from one of $K$ mixture components.\n\n## Conclusion\n\n**Gaussian Mixture Model** are ubiquitous probabilistic models for density estimation in machine learning [@titterington1985statistical]. It is widely used and known for its flexibility in understanding data distributions. They use gaussian distribution to figure out which component each data point likely to belong, and assigns probabilities to represent how data points are grouped. It helps us determine any uncertainties and variability in our data, providing insights on how data is distributed.\n\n## Acknowledgement {.appendix}\n\nThe \"Examples\" section above was taken from lecture notes written by Ramesh Sridharan.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}